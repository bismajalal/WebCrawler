{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MyCrawler.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nBq9mxC-N2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a827131-2299-493a-f878-6c4299ba9a90"
      },
      "source": [
        "import os \n",
        "import random\n",
        "import queue\n",
        "import heapq\n",
        "from urllib.parse import urlparse, urljoin\n",
        "import datetime\n",
        "import threading\n",
        "import time\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import httplib2\n",
        "import re\n",
        "import requests\n",
        "from urllib.parse import urlparse\n",
        "import urllib.robotparser\n",
        "import json\n",
        "import pickle"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Real_numbers\n",
            "140065837598464"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUIzB8XYKTQE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcca8774-f044-4d4a-bc67-780d6ca429e2"
      },
      "source": [
        "#assuming 5 is the lowest and 1 is the highest priority\n",
        "def prioritizer(URL,f):\n",
        "    '''Take URL and returns priority from 1 to F\n",
        "    Right now it like a stub function. \n",
        "    It will return a random number from 1 to f for given inputs.\n",
        "    f is the no. of front queues'''\n",
        "    #print(URL)\n",
        "    return random.randint(1,f)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/File:Linear_regression.svg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO7heCab-2Qc"
      },
      "source": [
        "FRONTIER\n",
        "\n",
        "Frontier should use the Mercator frontier design as discussed in lecture. Preferably it should be a class and should have the given functions. Prioritizer function is a stub right now, it will return a random number between 1 to f for given URL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A25laQ8-v9J"
      },
      "source": [
        "class frontier:\n",
        "# add the code for frontier here\n",
        "# should have functions __init__, get_URL, add_URLs, add_to_backqueue\n",
        "    \n",
        "    def __init__(self, seedURLs):\n",
        "      self.seedURLs = seedURLs\n",
        "    \n",
        "      #front queues, make a list of lists\n",
        "      #use list as a queue by using append() to push and pop(0) to pop\n",
        "      self.frontQueues = [[] for i in range(FRONTQUEUES)]\n",
        "\n",
        "      #back queues, make a list of lists\n",
        "      self.backQueues = [[] for i in range(BACKQUEUES)]\n",
        "\n",
        "      #dict holding mapping of BQ to Host\n",
        "      self.BQHostMapping = {}\n",
        "\n",
        "      #heap to maintain timestamps along with the BQ\n",
        "      self.heap = []\n",
        "\n",
        "    #returns first URL from highest priority FQ\n",
        "    def getFQURL(self):\n",
        "      for i in range(len(self.frontQueues)):\n",
        "        if len(self.frontQueues[i]) > 0:\n",
        "          return self.frontQueues[i].pop(0)\n",
        "\n",
        "    #returns URL from a BQ\n",
        "    def getBQURL(self):\n",
        "      #check if heap has root node then return the node\n",
        "      if len(self.heap) > 0:\n",
        "        root = heapq.heappop(self.heap)\n",
        "\n",
        "        #check the delaye needed\n",
        "        diff = root[0] - datetime.datetime.now()\n",
        "        sleepTime = diff.total_seconds()\n",
        "\n",
        "        #if timestamp is greater than currTime, sleep\n",
        "        if sleepTime > 0:\n",
        "          time.sleep(sleepTime)\n",
        "\n",
        "        #extract the URL from BQ\n",
        "        URL = self.backQueues[root[1]].pop(0)\n",
        "\n",
        "        #check if BQ become empty\n",
        "        #if BQ is empty, delete mapping from table and add a URL in BQ\n",
        "        if len(self.backQueues[root[1]]) == 0:\n",
        "          for key, value in self.BQHostMapping.items():\n",
        "            if value == root[1]:\n",
        "              del self.BQHostMapping[key]\n",
        "              break\n",
        "\n",
        "          self.addToBackQueue(root[1])\n",
        "        \n",
        "        #update timestamp heap for the BQ\n",
        "        delay = datetime.timedelta(0,15)\n",
        "        newTS = root[0] + delay\n",
        "        heapq.heappush(self.heap, [newTS, root[1]])\n",
        "\n",
        "        print(\"URL Removed from BQ:\", URL)\n",
        "\n",
        "        REMOVEDURLS.append(URL)\n",
        "        return URL\n",
        "        \n",
        "      else:\n",
        "        return None\n",
        "\n",
        "    \n",
        "    #takes a list of URLs and adds them to FQ based on priority\n",
        "    def addURLs(self, URL):\n",
        "      for i in range(len(URL)):\n",
        "        priority = prioritizer(URL[i], FRONTQUEUES)\n",
        "        \n",
        "        #decrement priority because index of list will start from 0 not 1\n",
        "        self.frontQueues[priority-1].append(URL[i])\n",
        "      \n",
        "      return\n",
        "    \n",
        "    #takes index of BQ and adds URLs unitll BQ is non-empty\n",
        "    def addToBackQueue(self, BQIndex):\n",
        "      \n",
        "      while len(self.backQueues[BQIndex]) == 0:\n",
        "\n",
        "        #get URL from highest priority FQ\n",
        "        URL = self.getFQURL()\n",
        "\n",
        "        #get hostname from the URL\n",
        "        host = urlparse(URL).hostname\n",
        "\n",
        "        #check if a BQ already has that host then add URL in that BQ\n",
        "        #else add URL in empty BQ and update mapping table\n",
        "        if host in self.BQHostMapping:\n",
        "          self.backQueues[self.BQHostMapping[host]].append(URL)\n",
        "        else:\n",
        "          self.backQueues[BQIndex].append(URL)\n",
        "          #update mapping table\n",
        "          self.BQHostMapping[host] = BQIndex\n",
        "        \n",
        "      return"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRgCDk4h--Yt"
      },
      "source": [
        "FILTER URLS\n",
        "\n",
        "Filter the URLS that are in robots.txt files of server and the have been already processed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eme1g0O__AHX"
      },
      "source": [
        "def fetch(myFrontier):\n",
        "  \n",
        "  URL = myFrontier.getBQURL()\n",
        "\n",
        "  if URL == None:\n",
        "    return None\n",
        "\n",
        "  #fetch URL\n",
        "  with urllib.request.urlopen(URL) as response:\n",
        "    html = response.read()\n",
        "    \n",
        "  #save the URL and content\n",
        "  CONTENT[URL]= html\n",
        "\n",
        "  return URL\n",
        "\n",
        "#get URLS in proper format if not already\n",
        "def fixURL(href, URL):\n",
        "  \n",
        "  #if URL is relative, make it absolute\n",
        "  if bool(urlparse(href).netloc) == False:\n",
        "    href = urljoin(URL, href)\n",
        "  \n",
        "  #some URLs start with // so fix them\n",
        "  if href.startswith(\"//\"):\n",
        "    href = \"https:\" + href\n",
        "\n",
        "  return href\n",
        "\n",
        "#takes the domain name and link and returns whether the link can be crawled or not\n",
        "def URLFilter(domain_name, href):\n",
        "    \n",
        "    #check if we already have the robot filename for the URL\n",
        "    if domain_name in ROBOTS:\n",
        "      robotFile = ROBOTS.get(domain_name)\n",
        "    else:\n",
        "      robotFile = \"https://\" + domain_name + \"/robots.txt\"\n",
        "      ROBOTS[domain_name] = robotFile\n",
        "    \n",
        "    #takes robotFile name and tells if a given link can be crawled\n",
        "    rp = urllib.robotparser.RobotFileParser()\n",
        "    rp.set_url(robotFile)\n",
        "    rp.read()\n",
        "    \n",
        "    canFetch = rp.can_fetch(\"*\", href)\n",
        "\n",
        "    return canFetch\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HudmAF-Pxrxy"
      },
      "source": [
        "# Run Crawler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoRGoE1ULCut"
      },
      "source": [
        "# Theard task\n",
        "def crawler_thread_task(myFrontier):\n",
        "\n",
        "  i = 0\n",
        "  while i < 1:\n",
        "    \n",
        "    URL = fetch(myFrontier)\n",
        "    if URL == None:\n",
        "      print(\"No URLs in the Frontier\")\n",
        "      return None\n",
        "\n",
        "    domain_name = urlparse(URL).netloc\n",
        "    soup = BeautifulSoup(requests.get(URL).content, \"html.parser\")\n",
        "    \n",
        "    linksExtracted = []\n",
        "    for a_tag in soup.findAll(\"a\"):\n",
        "      href = a_tag.attrs.get(\"href\")\n",
        "      if href == \"\" or href is None or href[0] == \"#\":      #remove empty and redundant tags\n",
        "        continue\n",
        "\n",
        "      #get URLS in proper format if not already\n",
        "      if href[0] != \"h\":\n",
        "        href = fixURL(href, URL)\n",
        "      \n",
        "      #only process the links in proper format\n",
        "      if href.startswith(\"https://\"):\n",
        "        canFetch = URLFilter(domain_name, href)\n",
        "\n",
        "        #add URL to list of links if it is not a duplicate\n",
        "        if canFetch == True:\n",
        "          #has the same URL already been extracted from the webpage\n",
        "          if href not in linksExtracted:\n",
        "            #is the new URL already crawled\n",
        "            if href not in REMOVEDURLS:\n",
        "              #print(threading.get_ident())\n",
        "              #print(href)\n",
        "              linksExtracted.append(href)\n",
        "    \n",
        "    #print(\"NEW LINKS\", *linksExtracted, sep=\"\\n\")\n",
        "    #add new URLS to the frontier\n",
        "    #myFrontier.addURLs(linksExtracted))\n",
        "      \n",
        "    i = i + 1\n",
        "\n",
        "  a_file = open(\"data.pkl\", \"wb\")\n",
        "  pickle.dump(CONTENT, a_file)\n",
        "  a_file.close()\n",
        "\n",
        "  print(\"1000 URLs Crawled. Stopping Now!\")\n",
        "\n",
        "  return None"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xZcQlHUuLErm",
        "outputId": "dc312701-02f5-47dc-bed4-09d9d9e6bed4"
      },
      "source": [
        "# Crawler Parameters\n",
        "BACKQUEUES= 3\n",
        "THREADS= BACKQUEUES*3\n",
        "FRONTQUEUES= 5\n",
        "WAITTIME= 15 ; # wait 15 seconds before fetching URLS from \n",
        "REMOVEDURLS = []\n",
        "CONTENT = {}\n",
        "ROBOTS = {}\n",
        "\n",
        "def main():\n",
        "    seedURLs = [ \"https://docs.oracle.com/en/\",\"https://www.oracle.com/corporate/\",\"https://www.csie.ntu.edu.tw/~cjlin/libsvm/index.html\"]\n",
        "    #\"https://docs.oracle.com/middleware/jet210/jet/index.html\",\"https://en.wikipedia.org/w/api.php\",\"https://en.wikipedia.org/api/\",\"https://en.wikipedia.org/wiki/Weka_(machine_learning)\" ]\n",
        "\n",
        "    myFrontier = frontier(seedURLs)\n",
        "\n",
        "    #fill the FQs\n",
        "    myFrontier.addURLs(seedURLs)\n",
        "\n",
        "    #right now all BQ are empty so fill them\n",
        "    #make timestamp heap for the backqueues as well\n",
        "    for i in range(BACKQUEUES):\n",
        "      myFrontier.addToBackQueue(i)\n",
        "      heapq.heappush(myFrontier.heap, [datetime.datetime.now(), i])\n",
        "    \n",
        "    #make a list of BACKQUEUES*3 threads\n",
        "    threads = []\n",
        "    for i in range(BACKQUEUES*2): \n",
        "        th = threading.Thread(target=crawler_thread_task, args=(myFrontier,))\n",
        "        threads.append(th)\n",
        "    \n",
        "    for i in range(BACKQUEUES*2):\n",
        "      threads[i].start()\n",
        "\n",
        "    for i in range(BACKQUEUES*2):\n",
        "      threads[i].join()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Motor_coordination#Muscle_synergies\n",
            "URL Removed from BQ: https://docs.oracle.com/en/\n",
            "No URLs in the Frontier\n",
            "No URLs in the Frontier\n",
            "1000 URLs Crawled. Stopping Now!\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Rule-based_machine_learning\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Learning_classifier_system\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Artificial_immune_system\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Rakesh_Agrawal_(computer_scientist)\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Tomasz_Imieli%C5%84ski\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Point-of-sale\n",
            "URL Removed from BQ: None\n",
            "No URLs in the Frontier\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Pricing\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Product_placement\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Market_basket_analysis\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Web_usage_mining\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Intrusion_detection\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Continuous_production\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Bioinformatics\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Sequence_mining\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Piecewise\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Logic_programming\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Entailment\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Inductive_programming\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Functional_programming\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Natural_language_processing\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Gordon_Plotkin\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Ehud_Shapiro\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Inductive_reasoning\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Mathematical_induction\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Statistical_model\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/File:Colored_neural_network.svg\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Neuron\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Brain\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Artificial_neuron\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Biological_neural_network\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Synapse\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Real_number\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Weight_(mathematics)\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Human_brain\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Biology\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Speech_recognition\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Machine_translation\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/Social_network\n",
            "140065837598464\n",
            "None https://en.wikipedia.org/wiki/General_game_playing\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-f96c5cd14a86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-47-f96c5cd14a86>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBACKQUEUES\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m       \u001b[0mthreads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}